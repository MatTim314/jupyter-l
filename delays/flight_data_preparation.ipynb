{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mate/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 11:19:35 WARN Utils: Your hostname, ces-shrd-1 resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/05/08 11:19:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/08 11:19:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/08 11:19:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/08 11:19:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab71331e-dd5a-4fe4-96d8-971c711c5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.select('DATE').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ea6c03-0380-4397-a6b4-c138cc21647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data cleanup and preparation\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 11:19:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "\n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "#df_select.show(n=5)\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n",
    "\n",
    "#grouped_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983d5e23-d23d-49cc-b55b-1427c1e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, col, lit, monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'grouped_df', transforming 'NAME' to lowercase and dropping duplicates based on the 'name' column\n",
    "grouped_df_lower = grouped_df.select(lower(col(\"NAME\")).alias(\"name\")).dropDuplicates(['name'])\n",
    "\n",
    "# For 'df_flightdelay', transforming 'DEPARTING_AIRPORT' to lowercase, casting it to string, and dropping duplicates based on the 'departing_airport' column\n",
    "df_flightdelay_lower = df_flightdelay.select(lower(col(\"DEPARTING_AIRPORT\")).alias(\"departing_airport\")).dropDuplicates(['departing_airport'])\n",
    "\n",
    "\n",
    "\n",
    "#join providing table that contain in the name column all distinct airports \n",
    "#from weather dataset and under departing_flight all distinc airports from delay dataset\n",
    "\n",
    "result_df = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    grouped_df_lower.alias(\"grouped\"),\n",
    "    (col(\"grouped.name\").contains(col(\"flight.departing_airport\"))),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"flight.departing_airport\").alias(\"departing_airport\"),\n",
    "    col(\"grouped.name\").alias(\"name\")\n",
    ")\n",
    "\n",
    "#result_df.show(n=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad8c016-135e-4c98-83d9-f36c28a72c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modify dataframe such that df_result will contain the airports matched on join \n",
    "#and enhanced results will contain the df of unmatched airports for each dataset\n",
    "\n",
    "# Identifying non-matched entries\n",
    "non_matched_flight = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.departing_airport == df_flightdelay_lower.departing_airport,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "non_matched_grouped = grouped_df_lower.alias(\"grouped\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.name == grouped_df_lower.name,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add a unique ID to each DataFrame to facilitate the outer join\n",
    "result_df = result_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_flight = non_matched_flight.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_grouped = non_matched_grouped.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# Perform the outer joins using the unique IDs, result_df is now composed of matched airports\n",
    "enhanced_result_df = result_df.join(non_matched_flight, \"id\", \"outer\" ).join(non_matched_grouped, \"id\", \"outer\" )\n",
    "enhanced_result_df = enhanced_result_df.drop(\"id\")\n",
    "\n",
    "# Show the enhanced DataFrame with additional columns\n",
    "#enhanced_result_df.printSchema()\n",
    "\n",
    "# Select columns, get rid of duplicates\n",
    "selected_columns = [col for col in enhanced_result_df.columns if col != 'name' and col != 'departing_airport'] + ['grouped.name'] + ['flight.departing_airport']\n",
    "\n",
    "#will contain unmatched airports for each dataset\n",
    "enhanced_result_df = enhanced_result_df.select(selected_columns)\n",
    "enhanced_result_df.drop('name','departing_airport')\n",
    "#enhanced_result_df.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7419b18-8499-4eb5-b65b-3b70dda46365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe that contains airports matched and unmatched result from the join\n",
    "# Rename columns in result_df\n",
    "result_df = result_df.withColumnRenamed(\"name\", \"weather_matched\") \\\n",
    "                     .withColumnRenamed(\"departing_airport\", \"delay_matched\")\n",
    "\n",
    "# Rename columns in enhanced_result_df\n",
    "enhanced_result_df = enhanced_result_df.withColumnRenamed(\"name\", \"weather_unmatched\") \\\n",
    "                                       .withColumnRenamed(\"departing_airport\", \"delay_unmatched\")\n",
    "\n",
    "# Optional: If you need to ensure the rows are matched by order, add an index column to each DataFrame\n",
    "result_df = result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "enhanced_result_df = enhanced_result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames on the index column\n",
    "matched_and_unmatched_airports = result_df.join(\n",
    "    enhanced_result_df,\n",
    "    on=\"index\",\n",
    "    how=\"outer\"  # Use \"outer\" to include all rows from both DataFrames\n",
    ")\n",
    "\n",
    "# Drop the index column as it's no longer needed after joining\n",
    "matched_and_unmatched_airports = matched_and_unmatched_airports.drop(\"index\", 'name', 'departing_airport')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e793e46b-e808-4fea-ad76-6a90cef070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the amount of airports for each clumn in the enhanced dataset dataframe \n",
    "\n",
    "#non_null_name_count = result_df.filter(col(\"name\").isNotNull()).count()\n",
    "#non_null_name_count1 = result_df.filter(col(\"departing_airport\").isNotNull()).count()\n",
    "#non_null_name_count2 = enhanced_result_df.filter(col(\"weather_unmatched\").isNotNull()).count()\n",
    "#non_null_name_count3 = enhanced_result_df.filter(col(\"delay_unmatched\").isNotNull()).count()\n",
    "#print(\"Number of non-null strings in the 'name' column:\", non_null_name_count, non_null_name_count1,non_null_name_count2, non_null_name_count3)\n",
    "\n",
    "# Display the filtered DataFrame and print the counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf47076e-52b6-4634-8e03-35ced09409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Open the text file and parse it line by line\n",
    "with open('./airports.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by comma to extract the needed parts\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        # Check if the line has enough parts to avoid index errors\n",
    "        if len(parts) >= 4:\n",
    "            # Extract and clean the desired parts\n",
    "            # Remove quotation marks and extra spaces if present\n",
    "            name = parts[1].strip('\"').strip()\n",
    "            city = parts[2].strip('\"').strip()\n",
    "            country = parts[3].strip('\"').strip()\n",
    "            \n",
    "            # Combine the first two parts into one column, and keep the country as the second column\n",
    "            combined = f\"{name}, {city}, {country}\"\n",
    "            data.append(combined)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_airports = pd.DataFrame(data, columns=['Airport and City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f08a73d9-8f5b-4070-9e51-0d8af85f4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c3c61f3-a54e-4edc-9d65-d4d2e84bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get all airport names into one table\n",
    "df_delay_unique_airport = df_flightdelay.select('DEPARTING_AIRPORT').distinct().withColumnRenamed(\"DEPARTING_AIRPORT\", \"airport\")\n",
    "df_weather_unique_airport = df_weather.select('NAME').distinct().withColumnRenamed(\"NAME\", \"airport\")\n",
    "\n",
    "df_union = df_delay_unique_airport.union(df_weather_unique_airport)\n",
    "\n",
    "def filter_out_useless_parts_of_string(airport_name):\n",
    "    useless_words = [\"international\", \"airport\", \"regional\"]\n",
    "\n",
    "    modified = airport_name.lower()\n",
    "    for word in useless_words:\n",
    "        modified = modified.replace(word,\"\")\n",
    "        \n",
    "    return modified\n",
    "    \n",
    "filter_string_udf = udf(filter_out_useless_parts_of_string, StringType())\n",
    "\n",
    "df_union = df_union.select(filter_string_udf(col('airport'))).withColumnRenamed(\"filter_out_useless_parts_of_string(airport)\", \"airport\")\n",
    "#df_union.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8392322f-bd5d-47ea-ad10-763847e59700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def get_matches(df1, col1, df2, col2, threshold=40):\n",
    "    # Convert each column to a list for processing, ensuring to drop NA values\n",
    "    list1 = df1[col1].dropna().tolist()\n",
    "    list2 = df2[col2].dropna().tolist()\n",
    "\n",
    "    # Find best matches with a score above the threshold\n",
    "    matches = []\n",
    "    for item in list1:\n",
    "        # Use process.extractOne to find the best match for each item from list1 in list2\n",
    "        best_match = process.extractOne(item, list2, scorer=fuzz.token_set_ratio)\n",
    "        if best_match and best_match[1] >= threshold:\n",
    "            matches.append((item, best_match[0], best_match[1]))\n",
    "\n",
    "    # Return matches as a DataFrame for better visualization\n",
    "    return pd.DataFrame(matches, columns=[col1, col2 + '_match', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "935bd86e-6047-4e87-8d53-330b4cbd3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add official airports using fuzzywuzzy\n",
    "# Assuming 'matched_and_unmatched_airports' is your PySpark DataFrame\n",
    "pandas_df = df_union.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Example usage (ensure df1 and df2 are already defined and loaded with your data)\n",
    "df_matches_airports = get_matches(pandas_df, 'airport', df_airports, 'Airport and City')\n",
    "\n",
    "spark_df_airports = spark.createDataFrame(df_matches_airports)\n",
    "# Show the DataFrame to verify conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5415602-ebd2-4bdd-a27b-13ec68b697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = spark_df_airports.select(\"airport\", \"Airport and City_match\")\n",
    "#joining_table.show(n=2)\n",
    "delay_table = df_flightdelay.withColumn(\"DEPARTING_AIRPORT\", filter_string_udf('DEPARTING_AIRPORT'))\n",
    "#delay_table.show(n=2)\n",
    "weather_table = df_day_column.withColumn(\"NAME\", filter_string_udf('NAME'))\n",
    "#weather_table.show(n=2)\n",
    "\n",
    "#previous_arprt_table = df_flightdelay.withColumn(\"PREVIOUS_AIRPORT\", filter_string_udf('PREVIOUS_AIRPORT'))\n",
    "#previous_arprt_joined = previous_arprt_table.join(joining_table, joining_table.airport == previous_arprt_table.PREVIOUS_AIRPORT, 'inner')\n",
    "\n",
    "delay_joined = delay_table.join(joining_table, joining_table.airport == delay_table.DEPARTING_AIRPORT, 'inner')\n",
    "weather_joined = weather_table.join(joining_table, joining_table.airport == weather_table.NAME, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe908b1-39e2-44d4-8c86-5408294eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#delay_joined.show(n=3)\n",
    "#weather_joined.show(n=3)\n",
    "\n",
    "# Convert integer columns in df1 to strings\n",
    "weather_joined = weather_joined.withColumn(\"MONTH\", col(\"MONTH\").cast(\"string\")) \\\n",
    "         .withColumn(\"DAY_OF_WEEK\", col(\"DAY_OF_WEEK\").cast(\"string\"))\n",
    "\n",
    "result_joined = delay_joined.join(weather_joined, [\"MONTH\",\"DAY_OF_WEEK\",\"Airport and City_match\"], 'inner')\n",
    "\n",
    "#result_joined.show(n=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c69994-b87c-4754-9200-b20cbcb50dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "# LATITUDE, LONGITUDE, STATION, MONTH, airport, normalized_name, NAME, _c0\n",
    "result_joined = result_joined.drop(\"LATITUDE\", \"LONGITUDE\", \"STATION\", \"MONTH\", \\\n",
    "                                   \"airport\", \"normalized_name\", \"NAME\", \"_c0\", \\\n",
    "                                   \"DATE\", \"AIRLINE_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRLINE\" \\\n",
    "                                   \"DEPARTING_AIRPORT\", \"PGTM\", \"WDF5\", \"WDF2\", \"WSF2\", \"WSF5\", \\\n",
    "                                   \"SN32\", \"SX32\", \"TOBS\",\"WESD\", \"PSUN\",\"TSUN\")\n",
    "\n",
    "df = result_joined\n",
    "#result_joined.count()\n",
    "#result_joined.select(\"CONCURRENT_FLIGHTS\").filter(col(\"CONCURRENT_FLIGHTS\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bc58762-a744-44fe-9c6e-8272a8e5e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"float\"))\n",
    "df = df.withColumn(\"PRCP\", df[\"PRCP\"].cast(\"float\"))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"SNOW\", when(col(\"TMIN\") > 3, 0).otherwise(col(\"SNOW\")))\n",
    "df = df.withColumn(\"SNOW\", when( \\\n",
    "                    (col(\"SNOW\").isNull() | (col(\"SNOW\") == '')) & (col(\"PRCP\") > 0), col(\"PRCP\") \\\n",
    "                               ).otherwise(lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cfb6937-44ac-49c8-b956-12837d92fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify temperatures, create new flag column EXTREME_WEATHER based on TMIN and TMAX and drop all others\n",
    "\n",
    "\n",
    "# Since TMAX and TMIN are strings, you need to convert them to integers before comparison\n",
    "df = df.withColumn(\"TMAX\", df[\"TMAX\"].cast(\"integer\"))\n",
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"integer\"))\n",
    "\n",
    "# Creating the EXTREME_WEATHER column based on the conditions provided\n",
    "df = df.withColumn(\"EXTREME_WEATHER\", \n",
    "                   when((col(\"TMAX\") > 40) | (col(\"TMIN\") < 0), 1)\n",
    "                   .otherwise(0))\n",
    "\n",
    "df = df.drop(\"TMIN\", \"TMAX\", \"TAVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2395a6b-e573-4f21-bcc3-61898a3f3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Replace all WT** with column which adds these extreme weather conditions into one value\n",
    "values_as_strings = [f\"WT{i:02}\" for i in range(1, 12)]\n",
    "#print(values_as_strings)\n",
    "#print(\"+\".join(values_as_strings))\n",
    "\n",
    "for column_name in values_as_strings:\n",
    "    df = df.withColumn(column_name, df[column_name].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c36c5ab-63ce-4591-96e2-7780413559a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f966b1-c944-4e85-bdc6-cfda4eede96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_wt_column = reduce(lambda a, b: a + b, [coalesce(col(c), lit(0)) for c in values_as_strings])\n",
    "\n",
    "df = df.withColumn('EXTREME_WEATHER_WT', total_wt_column)\n",
    "\n",
    "df = df.drop('WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16071707-3de0-41a5-80bb-f55cb3982655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7fdc75a-e3cc-481f-967d-469dff1dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Define a function to count nulls and empty strings\n",
    "def count_nulls_and_empties(df):\n",
    "    # Use aggregation to sum up each condition of being null or empty across all columns\n",
    "    exprs = [count(when(df[c].isNull() | (df[c] == \"\"), c)).alias(c) for c in df.columns]\n",
    "    return df.agg(*exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a1a8912-5e86-4fcc-8fda-2702c487b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values, for example AWND and PRCP\n",
    "\n",
    "from pyspark.sql.functions import avg, col, coalesce, month, median\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window spec partitioned by month\n",
    "window_spec = Window.partitionBy(month(\"DATE_NEW\"))\n",
    "\n",
    "# Assuming 'column_name' is the column with null values you want to fill\n",
    "avg_column = avg(col(\"AWND\")).over(window_spec)\n",
    "avg_prcp = avg(col(\"PRCP\")).over(window_spec)\n",
    "med_column = median(col(\"FLT_ATTENDANTS_PER_PASS\")).over(window_spec)\n",
    "\n",
    "\n",
    "# Replace nulls with the average of that month\n",
    "df = df.withColumn(\"AWND_filled\", coalesce(col(\"AWND\"), avg_column))\n",
    "df = df.withColumn(\"PRCP_filled\", coalesce(col(\"PRCP\"), avg_prcp))\n",
    "df = df.withColumn(\"DISTANCE_GROUP_filled\", coalesce(col(\"DISTANCE_GROUP\"), lit(\"1\")))\n",
    "df = df.withColumn(\"FLT_ATTENDANTS_PER_PASS_filled\", coalesce(col(\"FLT_ATTENDANTS_PER_PASS\"), med_column))\n",
    "\n",
    "\n",
    "df = df.drop(\"AWND\").withColumnRenamed(\"AWND_filled\", \"AWND\")\n",
    "df = df.drop(\"PRCP\").withColumnRenamed(\"PRCP_filled\", \"PRCP\")\n",
    "df = df.drop(\"DISTANCE_GROUP\").withColumnRenamed(\"DISTANCE_GROUP_filled\", \"DISTANCE_GROUP\")\n",
    "df = df.drop(\"FLT_ATTENDANTS_PER_PASS\").withColumnRenamed(\"FLT_ATTENDANTS_PER_PASS_filled\", \"FLT_ATTENDANTS_PER_PASS\")\n",
    "\n",
    "missing_values_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad0f433d-8a43-49d5-9797-dddb92475d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a5bfb2-6267-43d9-9a98-346b022ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Replace null values in the 'PREVIOUS_AIRPORT' column with empty strings\n",
    "df = df.withColumn('PREVIOUS_AIRPORT', when(col('PREVIOUS_AIRPORT').isNull(), '').otherwise(col('PREVIOUS_AIRPORT')))\n",
    "#df.show(n=2)\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "#nulls_and_empties_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a075780-d721-40c3-9517-ccffafbd1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "calculate_flights = df.select(\"CARRIER_NAME\", \"Airport and City_match\", \"DATE_NEW\")\n",
    "\n",
    "calculate_flights = calculate_flights.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "count = calculate_flights.groupBy(\"CARRIER_NAME\", \"Airport and City_match\", \"MONTH\").count()\n",
    "#print(count.show())\n",
    "\n",
    "count = count.groupBy(\"CARRIER_NAME\", \"Airport and City_match\") \\\n",
    "                   .agg(round(avg(\"count\")).alias(\"monthly_avg_count\"))\n",
    "#print(count.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237a0403-0565-4b6b-b853-bd1a77a38289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf = df.join(\n",
    "    count,\n",
    "    [\"CARRIER_NAME\", \"Airport and City_match\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "rsdf = rsdf.withColumn(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\",\\\n",
    "                       when(\\\n",
    "                           (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\").isNull() | (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\") == ''))\n",
    "                           , col(\"monthly_avg_count\")).otherwise(col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\")))\n",
    "\n",
    "rsdf = rsdf.drop(\"monthly_avg_count\")\n",
    "\n",
    "#rsdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf93c537-7fbd-4dc7-85cf-9e0c887ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rsdf\n",
    "\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "#nulls_and_empties_count.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a5c1ed2-4caa-469a-817a-d5f255941d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation to sum up each condition of being null or empty across all columns\n",
    "#exprs = [count(when(df[c].isNull() | (df[c] == \"\"), c)).alias(c) for c in df.columns]\n",
    "#df.agg(*exprs).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01aa0cc6-5c29-41b8-91a1-da199cf4b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Converting from numerical to nominal\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cae857cd-5074-426f-9b12-da9a6f0e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert precipitation from numerical to nominal\n",
    "\n",
    "# Calculate the quantile thresholds\n",
    "#thresholds = result_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "#result_df = result_df.withColumn(\n",
    "#    \"precip_category\",\n",
    "#    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "#    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "#    .otherwise(\"high\")\n",
    "#)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#result_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0676654e-66b7-41ae-a093-f10e7832b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform weekday from numerical to nominal\n",
    "\n",
    "# Weekday mapping dictionary\n",
    "month_dict = {\n",
    "    '1': 'Monday', '2': 'Tuesday', '3': 'Wednesday', '4': 'Thursday', \n",
    "    '5': 'Friday', '6': 'Saturday', '7': 'Sunday'}\n",
    "\n",
    "# Define the UDF to convert numerical months to names\n",
    "def convert_weekday_to_name(weekday):\n",
    "    return month_dict.get(str(weekday), \"Unknown\")\n",
    "\n",
    "convert_weekday_udf = udf(convert_weekday_to_name, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column with month names\n",
    "#df_with_months = result_df.withColumn(\"DAY_OF_WEEK_NAME\", convert_weekday_udf(result_df[\"DAY_OF_WEEK\"]))\n",
    "#df_with_months.show(n=1)\n",
    "#df_with_months = df_with_months.drop(\"DAY_OF_WEEK\").withColumnRenamed(\"DAY_OF_WEEK_NAME\", \"DAY_OF_WEEK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85d45649-18f7-41b7-bd6e-2ecee5b3df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") <= 100, \"Small\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 200, \"Medium\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 400, \"Large\")\n",
    "    .otherwise(\"Jumbo\")\n",
    ")\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "df = df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d43c8aee-bbab-4d38-a8d9-37cb1293e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane age into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") <= 10, \"New\")\n",
    "    .when(col(\"PLANE_AGE\") <= 20, \"Standard\")\n",
    "    .otherwise(\"Old\")\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "df = df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")\n",
    "\n",
    "\n",
    "this_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75474cbf-7e74-4b0f-a69d-d0543817ca7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = false)\n",
      " |-- PLANE_AGE: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c78b71-7896-4d72-89b5-b45bd52e3fc2",
   "metadata": {},
   "source": [
    "# Save cleaned and prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d17bdf95-1872-43f6-98c4-df7df9c9a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned and prepared data file to a csv\n",
    "df.write.csv('cleaned_flight_data.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
