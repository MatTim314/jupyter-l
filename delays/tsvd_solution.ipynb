{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9619a837-a193-4b16-ab13-404baa4ae797",
   "metadata": {},
   "source": [
    "# Data import\n",
    "- Set SPARK_HOME correctly before running the notebook (or comment it out)\n",
    "- These files should be in the same directory as this notebook:\n",
    "  - `full_data_flightdelay.csv`\n",
    "  - `airport_weather_2019.csv`\n",
    "  - `airports.txt` - Official airport names with their display names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hel/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 11:10:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Data import and cleanup\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febca9f-2bb7-4c8c-9b72-fffd7ed7fffe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data cleanup and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 11:10:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "# put date into same format\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "\n",
    "# add day of week and month column to weather\n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983d5e23-d23d-49cc-b55b-1427c1e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, col, lit, monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'grouped_df', transforming 'NAME' to lowercase and dropping duplicates based on the 'name' column\n",
    "grouped_df_lower = grouped_df.select(lower(col(\"NAME\")).alias(\"name\")).dropDuplicates(['name'])\n",
    "\n",
    "# For 'df_flightdelay', transforming 'DEPARTING_AIRPORT' to lowercase, casting it to string, and dropping duplicates based on the 'departing_airport' column\n",
    "df_flightdelay_lower = df_flightdelay.select(lower(col(\"DEPARTING_AIRPORT\")).alias(\"departing_airport\")).dropDuplicates(['departing_airport'])\n",
    "\n",
    "\n",
    "\n",
    "#join providing table that contain in the name column all distinct airports \n",
    "#from weather dataset and under departing_flight all distinc airports from delay dataset\n",
    "\n",
    "result_df = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    grouped_df_lower.alias(\"grouped\"),\n",
    "    (col(\"grouped.name\").contains(col(\"flight.departing_airport\"))),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"flight.departing_airport\").alias(\"departing_airport\"),\n",
    "    col(\"grouped.name\").alias(\"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3b430-e9d7-4d6c-9af1-d32c6730a8f5",
   "metadata": {},
   "source": [
    "### Modify dataframe such that `df_result` will contain the airports matched on join and enhanced results will contain the `df` of unmatched airports for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad8c016-135e-4c98-83d9-f36c28a72c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Identifying non-matched entries\n",
    "non_matched_flight = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.departing_airport == df_flightdelay_lower.departing_airport,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "non_matched_grouped = grouped_df_lower.alias(\"grouped\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.name == grouped_df_lower.name,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add a unique ID to each DataFrame to facilitate the outer join\n",
    "result_df = result_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_flight = non_matched_flight.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_grouped = non_matched_grouped.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# Perform the outer joins using the unique IDs, result_df is now composed of matched airports\n",
    "enhanced_result_df = result_df.join(non_matched_flight, \"id\", \"outer\" ).join(non_matched_grouped, \"id\", \"outer\" )\n",
    "enhanced_result_df = enhanced_result_df.drop(\"id\")\n",
    "\n",
    "\n",
    "# Select columns, get rid of duplicates\n",
    "selected_columns = [col for col in enhanced_result_df.columns if col != 'name' and col != 'departing_airport'] + ['grouped.name'] + ['flight.departing_airport']\n",
    "\n",
    "#will contain unmatched airports for each dataset\n",
    "enhanced_result_df = enhanced_result_df.select(selected_columns)\n",
    "enhanced_result_df.drop('name','departing_airport')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7419b18-8499-4eb5-b65b-3b70dda46365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe that contains airports matched and unmatched result from the join\n",
    "# Rename columns in result_df\n",
    "result_df = result_df.withColumnRenamed(\"name\", \"weather_matched\") \\\n",
    "                     .withColumnRenamed(\"departing_airport\", \"delay_matched\")\n",
    "\n",
    "# Rename columns in enhanced_result_df\n",
    "enhanced_result_df = enhanced_result_df.withColumnRenamed(\"name\", \"weather_unmatched\") \\\n",
    "                                       .withColumnRenamed(\"departing_airport\", \"delay_unmatched\")\n",
    "\n",
    "# Optional: If you need to ensure the rows are matched by order, add an index column to each DataFrame\n",
    "result_df = result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "enhanced_result_df = enhanced_result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames on the index column\n",
    "matched_and_unmatched_airports = result_df.join(\n",
    "    enhanced_result_df,\n",
    "    on=\"index\",\n",
    "    how=\"outer\"  # Use \"outer\" to include all rows from both DataFrames\n",
    ")\n",
    "\n",
    "# Drop the index column as it's no longer needed after joining\n",
    "matched_and_unmatched_airports = matched_and_unmatched_airports.drop(\"index\", 'name', 'departing_airport')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf47076e-52b6-4634-8e03-35ced09409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Open the text file and parse it line by line\n",
    "with open('./airports.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by comma to extract the needed parts\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        # Check if the line has enough parts to avoid index errors\n",
    "        if len(parts) >= 4:\n",
    "            # Extract and clean the desired parts\n",
    "            # Remove quotation marks and extra spaces if present\n",
    "            name = parts[1].strip('\"').strip()\n",
    "            city = parts[2].strip('\"').strip()\n",
    "            country = parts[3].strip('\"').strip()\n",
    "            \n",
    "            # Combine the first two parts into one column, and keep the country as the second column\n",
    "            combined = f\"{name}, {city}, {country}\"\n",
    "            data.append(combined)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_airports = pd.DataFrame(data, columns=['Airport and City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08a73d9-8f5b-4070-9e51-0d8af85f4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in /home/hel/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in /home/hel/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in /home/hel/.local/lib/python3.10/site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /home/hel/.local/lib/python3.10/site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c3c61f3-a54e-4edc-9d65-d4d2e84bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get all airport names into one table\n",
    "df_delay_unique_airport = df_flightdelay.select('DEPARTING_AIRPORT').distinct().withColumnRenamed(\"DEPARTING_AIRPORT\", \"airport\")\n",
    "df_weather_unique_airport = df_weather.select('NAME').distinct().withColumnRenamed(\"NAME\", \"airport\")\n",
    "\n",
    "df_union = df_delay_unique_airport.union(df_weather_unique_airport)\n",
    "\n",
    "def filter_out_useless_parts_of_string(airport_name):\n",
    "    useless_words = [\"international\", \"airport\", \"regional\"]\n",
    "\n",
    "    modified = airport_name.lower()\n",
    "    for word in useless_words:\n",
    "        modified = modified.replace(word,\"\")\n",
    "        \n",
    "    return modified\n",
    "    \n",
    "filter_string_udf = udf(filter_out_useless_parts_of_string, StringType())\n",
    "\n",
    "df_union = df_union.select(filter_string_udf(col('airport'))).withColumnRenamed(\"filter_out_useless_parts_of_string(airport)\", \"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8392322f-bd5d-47ea-ad10-763847e59700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def get_matches(df1, col1, df2, col2, threshold=40):\n",
    "    # Convert each column to a list for processing, ensuring to drop NA values\n",
    "    list1 = df1[col1].dropna().tolist()\n",
    "    list2 = df2[col2].dropna().tolist()\n",
    "\n",
    "    # Find best matches with a score above the threshold\n",
    "    matches = []\n",
    "    for item in list1:\n",
    "        # Use process.extractOne to find the best match for each item from list1 in list2\n",
    "        best_match = process.extractOne(item, list2, scorer=fuzz.token_set_ratio)\n",
    "        if best_match and best_match[1] >= threshold:\n",
    "            matches.append((item, best_match[0], best_match[1]))\n",
    "\n",
    "    # Return matches as a DataFrame for better visualization\n",
    "    return pd.DataFrame(matches, columns=[col1, col2 + '_match', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935bd86e-6047-4e87-8d53-330b4cbd3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add official airports using fuzzy matching\n",
    "# Assuming 'matched_and_unmatched_airports' is your PySpark DataFrame\n",
    "pandas_df = df_union.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Example usage (ensure df1 and df2 are already defined and loaded with your data)\n",
    "df_matches_airports = get_matches(pandas_df, 'airport', df_airports, 'Airport and City')\n",
    "\n",
    "spark_df_airports = spark.createDataFrame(df_matches_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5415602-ebd2-4bdd-a27b-13ec68b697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = spark_df_airports.select(\"airport\", \"Airport and City_match\")\n",
    "delay_table = df_flightdelay.withColumn(\"DEPARTING_AIRPORT\", filter_string_udf('DEPARTING_AIRPORT'))\n",
    "weather_table = df_day_column.withColumn(\"NAME\", filter_string_udf('NAME'))\n",
    "\n",
    "delay_joined = delay_table.join(joining_table, joining_table.airport == delay_table.DEPARTING_AIRPORT, 'inner')\n",
    "weather_joined = weather_table.join(joining_table, joining_table.airport == weather_table.NAME, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe908b1-39e2-44d4-8c86-5408294eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer columns in df1 to strings\n",
    "weather_joined = weather_joined.withColumn(\"MONTH\", col(\"MONTH\").cast(\"string\")) \\\n",
    "         .withColumn(\"DAY_OF_WEEK\", col(\"DAY_OF_WEEK\").cast(\"string\"))\n",
    "\n",
    "result_joined = delay_joined.join(weather_joined, [\"MONTH\",\"DAY_OF_WEEK\",\"Airport and City_match\"], 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b296f4-11a3-48a6-be48-12e5b3279e86",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2c69994-b87c-4754-9200-b20cbcb50dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE, LONGITUDE, STATION, MONTH, airport, normalized_name, NAME, _c0\n",
    "result_joined = result_joined.drop(\"LATITUDE\", \"LONGITUDE\", \"STATION\", \"MONTH\", \\\n",
    "                                   \"airport\", \"normalized_name\", \"NAME\", \"_c0\", \\\n",
    "                                   \"DATE\", \"AIRLINE_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRLINE\" \\\n",
    "                                   \"DEPARTING_AIRPORT\", \"PGTM\", \"WDF5\", \"WDF2\", \"WSF2\", \"WSF5\", \\\n",
    "                                   \"SN32\", \"SX32\", \"TOBS\",\"WESD\", \"PSUN\",\"TSUN\")\n",
    "\n",
    "df = result_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bc58762-a744-44fe-9c6e-8272a8e5e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"float\"))\n",
    "df = df.withColumn(\"PRCP\", df[\"PRCP\"].cast(\"float\"))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"SNOW\", when(col(\"TMIN\") > 3, 0).otherwise(col(\"SNOW\")))\n",
    "df = df.withColumn(\"SNOW\", when( \\\n",
    "                    (col(\"SNOW\").isNull() | (col(\"SNOW\") == '')) & (col(\"PRCP\") > 0), col(\"PRCP\") \\\n",
    "                               ).otherwise(lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cfb6937-44ac-49c8-b956-12837d92fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify temperatures, create new flag column EXTREME_WEATHER based on TMIN and TMAX and drop all others\n",
    "\n",
    "\n",
    "# Since TMAX and TMIN are strings, you need to convert them to integers before comparison\n",
    "df = df.withColumn(\"TMAX\", df[\"TMAX\"].cast(\"integer\"))\n",
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"integer\"))\n",
    "\n",
    "# Creating the EXTREME_WEATHER column based on the conditions provided\n",
    "df = df.withColumn(\"EXTREME_WEATHER\", \n",
    "                   when((col(\"TMAX\") > 40) | (col(\"TMIN\") < 0), 1)\n",
    "                   .otherwise(0))\n",
    "\n",
    "df = df.drop(\"TMIN\", \"TMAX\", \"TAVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2395a6b-e573-4f21-bcc3-61898a3f3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Replace all WT** with column which adds these extreme weather conditions into one value\n",
    "values_as_strings = [f\"WT{i:02}\" for i in range(1, 12)]\n",
    "\n",
    "for column_name in values_as_strings:\n",
    "    df = df.withColumn(column_name, df[column_name].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f966b1-c944-4e85-bdc6-cfda4eede96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_wt_column = reduce(lambda a, b: a + b, [coalesce(col(c), lit(0)) for c in values_as_strings])\n",
    "\n",
    "df = df.withColumn('EXTREME_WEATHER_WT', total_wt_column)\n",
    "\n",
    "df = df.drop('WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16071707-3de0-41a5-80bb-f55cb3982655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d0cc7-7a65-4e04-9cac-5a2dda17dedc",
   "metadata": {},
   "source": [
    "### Count null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7fdc75a-e3cc-481f-967d-469dff1dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Define a function to count nulls and empty strings\n",
    "def count_nulls_and_empties(df):\n",
    "    # Use aggregation to sum up each condition of being null or empty across all columns\n",
    "    exprs = [count(when(df[c].isNull() | (df[c] == \"\"), c)).alias(c) for c in df.columns]\n",
    "    return df.agg(*exprs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf53b19-19f5-4b32-9693-31088985854f",
   "metadata": {},
   "source": [
    "## Fill in missing values, for example AWND and PRCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a1a8912-5e86-4fcc-8fda-2702c487b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values, for example AWND and PRCP\n",
    "\n",
    "from pyspark.sql.functions import avg, col, coalesce, month, median\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window spec partitioned by month\n",
    "window_spec = Window.partitionBy(month(\"DATE_NEW\"))\n",
    "\n",
    "# Assuming 'column_name' is the column with null values you want to fill\n",
    "avg_column = avg(col(\"AWND\")).over(window_spec)\n",
    "avg_prcp = avg(col(\"PRCP\")).over(window_spec)\n",
    "med_column = median(col(\"FLT_ATTENDANTS_PER_PASS\")).over(window_spec)\n",
    "\n",
    "\n",
    "# Replace nulls with the average of that month\n",
    "df = df.withColumn(\"AWND_filled\", coalesce(col(\"AWND\"), avg_column))\n",
    "df = df.withColumn(\"PRCP_filled\", coalesce(col(\"PRCP\"), avg_prcp))\n",
    "df = df.withColumn(\"DISTANCE_GROUP_filled\", coalesce(col(\"DISTANCE_GROUP\"), lit(\"1\")))\n",
    "df = df.withColumn(\"FLT_ATTENDANTS_PER_PASS_filled\", coalesce(col(\"FLT_ATTENDANTS_PER_PASS\"), med_column))\n",
    "\n",
    "\n",
    "df = df.drop(\"AWND\").withColumnRenamed(\"AWND_filled\", \"AWND\")\n",
    "df = df.drop(\"PRCP\").withColumnRenamed(\"PRCP_filled\", \"PRCP\")\n",
    "df = df.drop(\"DISTANCE_GROUP\").withColumnRenamed(\"DISTANCE_GROUP_filled\", \"DISTANCE_GROUP\")\n",
    "df = df.drop(\"FLT_ATTENDANTS_PER_PASS\").withColumnRenamed(\"FLT_ATTENDANTS_PER_PASS_filled\", \"FLT_ATTENDANTS_PER_PASS\")\n",
    "\n",
    "missing_values_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad0f433d-8a43-49d5-9797-dddb92475d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a5bfb2-6267-43d9-9a98-346b022ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Replace null values in the 'PREVIOUS_AIRPORT' column with empty strings\n",
    "df = df.withColumn('PREVIOUS_AIRPORT', when(col('PREVIOUS_AIRPORT').isNull(), '').otherwise(col('PREVIOUS_AIRPORT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a075780-d721-40c3-9517-ccffafbd1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "calculate_flights = df.select(\"CARRIER_NAME\", \"Airport and City_match\", \"DATE_NEW\")\n",
    "\n",
    "calculate_flights = calculate_flights.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "count = calculate_flights.groupBy(\"CARRIER_NAME\", \"Airport and City_match\", \"MONTH\").count()\n",
    "\n",
    "count = count.groupBy(\"CARRIER_NAME\", \"Airport and City_match\") \\\n",
    "                   .agg(round(avg(\"count\")).alias(\"monthly_avg_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1ae43-17f9-4613-836a-be06210d514e",
   "metadata": {},
   "source": [
    "## Join into table combining delays with weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "237a0403-0565-4b6b-b853-bd1a77a38289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf = df.join(\n",
    "    count,\n",
    "    [\"CARRIER_NAME\", \"Airport and City_match\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "rsdf = rsdf.withColumn(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\",\\\n",
    "                       when(\\\n",
    "                           (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\").isNull() | (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\") == ''))\n",
    "                           , col(\"monthly_avg_count\")).otherwise(col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\")))\n",
    "\n",
    "rsdf = rsdf.drop(\"monthly_avg_count\")\n",
    "\n",
    "#rsdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf93c537-7fbd-4dc7-85cf-9e0c887ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01aa0cc6-5c29-41b8-91a1-da199cf4b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Converting from numerical to nominal\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae857cd-5074-426f-9b12-da9a6f0e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert precipitation from numerical to nominal\n",
    "\n",
    "# Calculate the quantile thresholds\n",
    "#thresholds = result_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "#result_df = result_df.withColumn(\n",
    "#    \"precip_category\",\n",
    "#    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "#    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "#    .otherwise(\"high\")\n",
    "#)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#result_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1410c-4231-4c65-a623-5e18f0a6c022",
   "metadata": {},
   "source": [
    "## Transform numerical to nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0676654e-66b7-41ae-a093-f10e7832b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform weekday from numerical to nominal\n",
    "\n",
    "# Weekday mapping dictionary\n",
    "month_dict = {\n",
    "    '1': 'Monday', '2': 'Tuesday', '3': 'Wednesday', '4': 'Thursday', \n",
    "    '5': 'Friday', '6': 'Saturday', '7': 'Sunday'}\n",
    "\n",
    "# Define the UDF to convert numerical months to names\n",
    "def convert_weekday_to_name(weekday):\n",
    "    return month_dict.get(str(weekday), \"Unknown\")\n",
    "\n",
    "convert_weekday_udf = udf(convert_weekday_to_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85d45649-18f7-41b7-bd6e-2ecee5b3df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") <= 100, \"Small\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 200, \"Medium\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 400, \"Large\")\n",
    "    .otherwise(\"Jumbo\")\n",
    ")\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "df = df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d43c8aee-bbab-4d38-a8d9-37cb1293e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane age into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") <= 10, \"New\")\n",
    "    .when(col(\"PLANE_AGE\") <= 20, \"Standard\")\n",
    "    .otherwise(\"Old\")\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "df = df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")\n",
    "\n",
    "\n",
    "this_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75474cbf-7e74-4b0f-a69d-d0543817ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = false)\n",
      " |-- PLANE_AGE: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c78b71-7896-4d72-89b5-b45bd52e3fc2",
   "metadata": {},
   "source": [
    "## Saving cleaned and prepared data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d17bdf95-1872-43f6-98c4-df7df9c9a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save cleaned and prepared data file to a csv\n",
    "df.write.csv('cleaned_flight_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43463f1b-1d27-4921-9f09-7a5c661ac820",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f34b2-2b81-45c3-b07f-3f758faae8c4",
   "metadata": {},
   "source": [
    "## Load cleaned data from csv\n",
    "- Data from the cleanup and preparation was saved into a csv to avoid repeated computations"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 38,
>>>>>>> eddb2a15dfd81f3b36c4d30f315ee4a53a1a53f1
   "id": "5d8fee67-856c-414a-99fd-a97e8ca17b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "24/05/09 10:59:03 WARN Utils: Your hostname, ces-shrd-1 resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/05/09 10:59:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/09 10:59:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 1:=======================================================> (31 + 1) / 32]\r"
=======
      "24/05/09 11:12:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 57:==============================================>           (4 + 1) / 5]\r"
>>>>>>> eddb2a15dfd81f3b36c4d30f315ee4a53a1a53f1
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- DEP_DEL15: integer (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: integer (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: integer (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: integer (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: double (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: integer (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: integer (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: double (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: double (nullable = true)\n",
      " |-- SNWD: double (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = true)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = true)\n",
      " |-- AWND: double (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: integer (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: double (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Data analysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "loaded_df = spark.read.csv(\"cleaned_flight_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "loaded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b03923-c2c3-424f-9394-1837214a6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df checkpoint\n",
    "df = loaded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc194694-b3a5-4626-8a03-5e31f579ea16",
   "metadata": {},
   "source": [
    "## Create a 10% sample\n",
    "- Computations on the entire data set would take too much time to work with sensibly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c80c9058-4c13-4ea7-9510-16256133e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the fraction for each unique value in 'DISTR' to maintain distribution\n",
    "fractions = df.select(\"DEP_DEL15\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "fractions = {x: 0.01 for x in fractions}  # 1% sample\n",
    "\n",
    "# Perform stratified sampling\n",
    "sampled_df = df.stat.sampleBy(\"DEP_DEL15\", fractions, seed=1)\n",
    "\n",
    "# Show the result or perform further analysis\n",
    "#sampled_df.show()\n",
    "#sampled_df.count() ~ 150 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa04c4bc-45a4-45e1-b2a5-c76a4c924f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Df checkpoint\n",
    "df = sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642a3af-4cb0-4239-b1a9-c7d266adc241",
   "metadata": {},
   "source": [
    "### Plotting of nominal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c809-26e4-47ae-aa85-e2fb60f4d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For better visual aesthetics\n",
    "\n",
    "# Setting a style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of nominal columns\n",
    "nominal_columns = [\n",
    "    \"CARRIER_NAME\",\n",
    "    \"Airport and City_match\",\n",
    "    \"DEP_TIME_BLK\",\n",
    "    \"DEPARTING_AIRPORT\",\n",
    "    \"PREVIOUS_AIRPORT\",\n",
    "    \"DATE_NEW\",\n",
    "    \"PLANE_AGE\"\n",
    "]\n",
    "\n",
    "# Create a figure to hold the subplots\n",
    "fig, axes = plt.subplots(nrows=len(nominal_columns), figsize=(12, 6 * len(nominal_columns)))  # Adjusted size for better fit\n",
    "\n",
    "# Loop through each nominal column and create a histogram\n",
    "for i, column in enumerate(nominal_columns):\n",
    "    # Count the occurrences of each category in the column\n",
    "    category_counts = df.groupBy(column).count().toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "    # If there are many categories, you might want to limit the number of categories displayed\n",
    "    if len(category_counts) > 20:  # Adjust the threshold as needed\n",
    "        category_counts = category_counts.head(20)  # Only show top 20 categories\n",
    "    \n",
    "    # Plotting the histogram for the column\n",
    "    sns.barplot(x=column, y='count', data=category_counts, ax=axes[i], palette='viridis')  # Using seaborn for a better-looking plot\n",
    "    axes[i].set_title(f'Histogram of {column}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Counts')\n",
    "    axes[i].tick_params(axis='x', rotation=45)  # Rotate labels to prevent overlap\n",
    "\n",
    "# Adjust layout to prevent overlap of subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#\n",
    "#CARRIER_NAME: 7.594988367110728e-05\n",
    "#Information Gain for Airport and City_match: 8.950570449908421e-05\n",
    "#Information Gain for DEP_TIME_BLK: 0.00044215567622180735\n",
    "#Information Gain for DEPARTING_AIRPORT: 8.933648488036738e-05\n",
    "#Information Gain for PREVIOUS_AIRPORT: 0.00036900671974516436\n",
    "#Information Gain for NUMBER_OF_SEATS: 0.00012451283453875778            \n",
    "#Information Gain for PLANE_AGE: -0.00014759181821908524"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f4c66-8846-4f0f-8d3f-b843dcd8b8db",
   "metadata": {},
   "source": [
    "### Cast numeric columns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "079b9472-5653-4263-8129-4f045d18ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "     \"DEP_DEL15\", \"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\",\n",
    "    \"AIRPORT_FLIGHTS_MONTH\",\n",
    "    \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\",\n",
    "    \"GROUND_SERV_PER_PASS\", \"SNOW\", \"EXTREME_WEATHER\", \"EXTREME_WEATHER_WT\",\n",
    "    \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\",\"SNWD\"\n",
    "]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    df_stat = df.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "df = df_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37914d-7006-4883-aca5-fa3f8c350e76",
   "metadata": {},
   "source": [
    "### Information gain calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f80505-7af2-462a-be76-ecc183bec053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import log2\n",
    "\n",
    "def entropy(probability):\n",
    "    \"\"\"Calculate the entropy based on probability.\"\"\"\n",
    "    return -probability * log2(probability)\n",
    "\n",
    "def calculate_entropy(df, column):\n",
    "    \"\"\"Calculate the entropy of a given DataFrame column.\"\"\"\n",
    "    total_count = df.count()\n",
    "    probabilities = df.groupBy(column).agg((count('*') / total_count).alias('probability'))\n",
    "    entropy_udf = udf(entropy, DoubleType())\n",
    "    entropy_df = probabilities.withColumn('entropy_part', entropy_udf('probability'))\n",
    "    total_entropy = entropy_df.agg(sqlsum('entropy_part')).collect()[0][0]\n",
    "    return total_entropy if total_entropy is not None else 0\n",
    "\n",
    "def information_gain(df, attribute, target):\n",
    "    \"\"\"Calculate the information gain of a column with respect to the target.\"\"\"\n",
    "    total_entropy = calculate_entropy(df, target)\n",
    "    total_count = df.count()\n",
    "\n",
    "    attribute_values = df.select(attribute).distinct().collect()\n",
    "\n",
    "    conditional_entropy_sum = 0\n",
    "    for value_row in attribute_values:\n",
    "        value = value_row[0]\n",
    "        subset_df = df.filter(col(attribute) == value)\n",
    "        subset_count = subset_df.count()\n",
    "        \n",
    "        # Ensure subset has sufficient data to compute entropy\n",
    "        if subset_count > 0:\n",
    "            subset_entropy = calculate_entropy(subset_df, target)\n",
    "            conditional_entropy_sum += (subset_count / total_count) * subset_entropy\n",
    "\n",
    "    return total_entropy - conditional_entropy_sum\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nominal_columns = [\"NUMBER_OF_SEATS\", \"PLANE_AGE\"]\n",
    "\n",
    "information_gains = {}\n",
    "for column in nominal_columns:\n",
    "    ig = information_gain(df, column, 'DEP_DEL15')  # Assuming DEP_DEL15 is your target column\n",
    "    information_gains[column] = ig\n",
    "    print(f'Information Gain for {column}: {ig}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca66019-a8a0-4878-bc57-40cc5d3137f1",
   "metadata": {},
   "source": [
    "# Preparation of data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e231f8f0-c794-4033-8b68-2bd82e915141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df checkpoint\n",
    "checkpoint_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a089d2b5-1945-4bc7-9110-41e8bba691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"DEPARTING_AIRPORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67476f-0f3c-489d-8cf4-8194079714cd",
   "metadata": {},
   "source": [
    "### Transforming nominal data into numerical\n",
    "- Models generally prefer working with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ea70e03-08a7-41bc-83c2-79297c4a3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df checkpoint\n",
    "checkpoint_df_nom_to_num = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49816900-c826-435a-a3c3-c652208b9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = checkpoint_df_nom_to_num\n",
    "# Plane age into numerical\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") == \"New\", 1)\n",
    "    .when(col(\"PLANE_AGE\") == \"Standard\", 2)\n",
    "    .otherwise(3)\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "df = df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f15198f-02e3-423a-95a3-1b9fbfc5d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into numerical category\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") == \"Small\", 1)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Medium\", 2)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Large\", 3)\n",
    "    .otherwise(4)\n",
    ")\n",
    "\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "df = df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c979958c-96f1-4feb-8bcc-c5ac5bc30b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK into numerical form\n",
    "from pyspark.sql.functions import col, radians, cos, sin\n",
    "\n",
    "df = df.withColumn(\"day_cos\", cos(radians(col(\"DAY_OF_WEEK\") * (360/7)))+1)\n",
    "df = df.withColumn(\"day_sin\", sin(radians(col(\"DAY_OF_WEEK\") * (360/7)))+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d5eab-9bd5-478e-8e8f-8ae0cf301826",
   "metadata": {},
   "source": [
    "## Indexing & encoding nominal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6562d3f3-e4b5-427c-a267-b888fbc5a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df checkpoint\n",
    "checkpoint_df_indx_encd = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc1abdd3-3841-4a5d-a2a4-53840b804fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- DEP_DEL15: float (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: float (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: float (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: float (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: float (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: float (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: float (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: float (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: float (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = true)\n",
      " |-- EXTREME_WEATHER_WT: float (nullable = true)\n",
      " |-- AWND: float (nullable = true)\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- DISTANCE_GROUP: float (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: float (nullable = true)\n",
      " |-- PLANE_AGE: integer (nullable = false)\n",
      " |-- NUMBER_OF_SEATS: float (nullable = false)\n",
      " |-- day_cos: float (nullable = true)\n",
      " |-- day_sin: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small preprocessing optimizations\n",
    "df.drop(\"SNWD\")\n",
    "#cast to float what can be casted\n",
    "float_columns = [\"DEP_DEL15\", \"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"AIRPORT_FLIGHTS_MONTH\",\n",
    "                 \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\", \"GROUND_SERV_PER_PASS\",\n",
    "                 \"SNOW\", \"EXTREME_WEATHER_WT\", \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\", \"day_cos\",\"day_sin\",\"NUMBER_OF_SEATS\",\"AVG_MONTHLY_PASS_AIRLINE\"]\n",
    "\n",
    "for col_name in float_columns:\n",
    "    df = df.withColumn(col_name, df[col_name].cast('float'))\n",
    "\n",
    "# Check the schema of the new DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34276217-f159-4dd6-89a2-aca3589ec084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexing and Encoding for 'CARRIER_NAME'\n",
    "carrier_name_indexer = StringIndexer(inputCol=\"CARRIER_NAME\", outputCol=\"CARRIER_NAME_Index\",handleInvalid=\"skip\")\n",
    "carrier_name_encoder = OneHotEncoder(inputCol=\"CARRIER_NAME_Index\", outputCol=\"CARRIER_NAME_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'PREVIOUS_AIRPORT'\n",
    "#prev_airport_indexer = StringIndexer(inputCol=\"PREVIOUS_AIRPORT\", outputCol=\"PREVIOUS_AIRPORT_Index\")\n",
    "#prev_airport_encoder = OneHotEncoder(inputCol=\"PREVIOUS_AIRPORT_Index\", outputCol=\"PREVIOUS_AIRPORT_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'Airport and City_match'\n",
    "city_match_indexer = StringIndexer(inputCol=\"Airport and City_match\", outputCol=\"Airport_and_City_match_Index\",handleInvalid=\"skip\")\n",
    "city_match_encoder = OneHotEncoder(inputCol=\"Airport_and_City_match_Index\", outputCol=\"Airport_and_City_match_Encoded\")\n",
    "\n",
    "\n",
    "#removed ground per pass,\"AIRPORT_FLIGHTS_MONTH\"\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ \"Airport_and_City_match_Encoded\", \"CARRIER_NAME_Encoded\"] + [\"SNOW\",\"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"NUMBER_OF_SEATS\",\n",
    "               \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\",\n",
    "               \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\", \"day_cos\",\"day_sin\", \"EXTREME_WEATHER_WT\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Pipeline all preprocessing steps\n",
    "pipeline = Pipeline(stages=[\n",
    "      city_match_indexer, city_match_encoder, carrier_name_indexer, carrier_name_encoder, assembler\n",
    "])\n",
    "\n",
    "# Fit and Transform\n",
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed82a8e-51af-468a-a51b-d97da567c99f",
   "metadata": {},
   "source": [
    "## Feature column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63d63e49-3717-49d1-a0f9-ecc1cd8025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkpoint_ftr_clmn = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89e15029-e279-406b-9451-39fa03ca6482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_df = model.transform(df)\n",
    "transformed_df.drop(\"PREVIOUS_AIRPORT\",\"CARRIER_NAME\",\"Airport and City_match\",\"DAY_OF_WEEK\")\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert DATE_NEW to DateType\n",
    "transformed_df = transformed_df.withColumn(\"DATE_NEW\", to_date(\"DATE_NEW\"))\n",
    "\n",
    "# Convert EXTREME_WEATHER to FloatType (assuming it's a binary indicator)\n",
    "transformed_df = transformed_df.withColumn(\"EXTREME_WEATHER\", col(\"EXTREME_WEATHER\").cast(\"float\"))\n",
    "k_means_df = transformed_df.select(\"features\")\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "def check_features_format(df):\n",
    "\n",
    "    # Check if the 'features' column exists in the DataFrame\n",
    "    if 'features' not in df.columns:\n",
    "        print(\"Error: 'features' column not found in the DataFrame.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if all values in the 'features' column are SparseVectors\n",
    "    all_sparse = df.select('features').rdd.map(lambda row: isinstance(row.features, SparseVector)).reduce(lambda x, y: x and y)\n",
    "    \n",
    "    return all_sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as dpd\n",
    "\n",
    "df = k_means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1293295a-d532-4096-a775-4bd333e147be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150251"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e7a1e-87c0-463b-b3db-fff59de90531",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ec8bd-94cc-4679-97c3-e905efabed4c",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bdb739a-44f3-4c3a-a460-70010ac55647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with feature column\n",
    "data_with_feature_column_df = transformed_df\n",
    "feature_column_only = transformed_df.select(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8eafc-7530-48b8-bf74-d7836b389e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_feature_column_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525470a-367a-4c73-98fb-d733b9cabfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure data does not contains values incompatible with models computations\n",
    "def count_negative_vals(df):\n",
    "    # Use aggregation to sum up each condition of being null or empty across all columns\n",
    "    exprs = [count(when((df[c] < 0), c)).alias(c) for c in df.columns]\n",
    "    return df.agg(*exprs)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "# Define a UDF to check minimum values in vectors\n",
    "min_element = udf(lambda v: float(min(v)), FloatType())\n",
    "data_with_feature_column_df.withColumn(\"min_feature\", min_element(\"features\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d43989ca-ac48-442d-b32a-c40677fff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the DataFrame into training (60%) and test (40%) sets\n",
    "def split_data(df):\n",
    "    train_data, test_data = df.randomSplit([0.6, 0.4], seed=1234)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Show the size of each set\n",
    "#print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "#print(\"Testing Dataset Count: \" + str(test_data.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627da45b-4b28-4795-9817-494e0536b319",
   "metadata": {},
   "source": [
    "## Decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204d775-8cd3-437a-92f6-b695dea8093f",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c90322a-40a2-4840-b561-3537b080a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(data_with_feature_column_df.select(\"features\", \"DEP_DEL15\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92fae9-ccb9-408a-bd3d-7580a6669800",
   "metadata": {},
   "source": [
    "### Fit, train, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9875691a-bf62-48a8-b80a-4cecb0f14979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hel/.local/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 93:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.808369\n",
      "Confusion Matrix:\n",
      "[[7749.   30.]\n",
      " [1811.   17.]]\n",
      "Precision: 0.8105648535564853\n",
      "Recall: 0.9961434631700733\n",
      "F1 Score: 0.7271987298433047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Decision Tree setup\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"DEP_DEL15\")\n",
    "dt_model = dt.fit(train_data)\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Compute confusion matrix\n",
    "rdd = predictions.select(col(\"prediction\"), col(\"DEP_DEL15\").cast(\"double\")).rdd\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d294b2-0882-4dab-a597-a8e9ef1d366b",
   "metadata": {},
   "source": [
    "## K-means model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61c309-982a-4772-b575-6fe139a756c1",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dc2dab7e-39f2-440f-a1c4-240de98631d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_column_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8f01b-9327-498d-8481-c8efaf60b89a",
   "metadata": {},
   "source": [
    "### Fit, train, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "beffd78e-acd4-4ca0-9b59-64a3592a34ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------+\n",
      "|            features|prediction|     distance|\n",
      "+--------------------+----------+-------------+\n",
      "|(80,[17,54,68,69,...|         1| 4.5613986E10|\n",
      "|(80,[19,52,68,69,...|         1|1.52854518E10|\n",
      "|(80,[25,52,68,69,...|         1|  7.8787425E9|\n",
      "|(80,[0,54,68,69,7...|         0| 8.8941068E11|\n",
      "|(80,[45,54,68,69,...|         1|2.00553923E11|\n",
      "|(80,[19,51,68,69,...|         1|1.52851005E10|\n",
      "|(80,[2,51,68,69,7...|         0| 1.10492576E8|\n",
      "|(80,[3,55,68,69,7...|         0|  5.0915384E9|\n",
      "|(80,[17,54,68,69,...|         1| 4.5613982E10|\n",
      "|(80,[2,59,68,69,7...|         0| 1.08633472E8|\n",
      "|(80,[17,54,68,69,...|         1| 4.5613986E10|\n",
      "|(80,[25,51,68,69,...|         1|  7.8783437E9|\n",
      "|(80,[45,58,68,69,...|         1|2.00553857E11|\n",
      "|(80,[45,64,68,69,...|         1|2.00553972E11|\n",
      "|(80,[45,51,68,69,...|         1|2.00553955E11|\n",
      "|(80,[3,55,68,69,7...|         0|  5.0915384E9|\n",
      "|(80,[25,55,68,69,...|         1|  7.8797036E9|\n",
      "|(80,[8,52,68,69,7...|         0|1.51877663E10|\n",
      "|(80,[25,63,68,69,...|         1|  7.8798653E9|\n",
      "|(80,[17,63,68,69,...|         1| 4.5615309E10|\n",
      "+--------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1188:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|            features|prediction|    distance|\n",
      "+--------------------+----------+------------+\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "|(80,[0,53,68,69,7...|         0|8.8955355E11|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, FloatType, ArrayType\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import sqrt, pow, col\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "\n",
    "kmeans = KMeans(k=4,seed=17399)\n",
    "# Get predictions and cluster centers\n",
    "centers = model.clusterCenters()\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# UDF to calculate the Euclidean distance from the nearest cluster center\n",
    "def distance_to_center(features, centers):\n",
    "    best_center = centers[min(range(len(centers)), key=lambda i: features.squared_distance(centers[i]))]\n",
    "    return float(features.squared_distance(best_center))\n",
    "\n",
    "distance_udf = udf(lambda features: distance_to_center(features, centers), FloatType())\n",
    "predictions = predictions.withColumn(\"distance\", distance_udf(col(\"features\")))\n",
    "\n",
    "# Show potential anomalies (e.g., distances greater than a threshold)\n",
    "distance_threshold = predictions.approxQuantile(\"distance\", [0.95], 0.05)  # 95th percentile with 5% relative error\n",
    "anomalies = predictions.filter(col(\"distance\") >= distance_threshold[0])\n",
    "\n",
    "predictions.show()\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffa650-8ca5-4381-adde-e0c32261373e",
   "metadata": {},
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf798f-09d1-4c58-a4ba-872811d4b3a9",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0ba27e06-5a3a-4efe-8618-da9c46d28450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(data_with_feature_column_df.select(\"features\", \"DEP_DEL15\").withColumnRenamed(\"DEP_DEL15\", \"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f81ff-e036-4241-aa52-1b504a39fe1d",
   "metadata": {},
   "source": [
    "### Fit, train, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "93845994-30e5-453c-a8d2-3c65ee06984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1159:====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5577150865017267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd30c1a-b11b-4af1-936e-96fcb956cbd3",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6eeac5-dce0-434e-95b4-b3e0d16b61c7",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e9cd6356-a8a1-4484-9f2e-8e905eed7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(data_with_feature_column_df.select(\"features\", \"DEP_DEL15\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa3d5-86d4-4ba4-b927-30fd956835b9",
   "metadata": {},
   "source": [
    "### Fit, train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "76577fe5-c96a-49db-bce4-38465beb7b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 13:46:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/08 13:46:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize the LinearSVC model\n",
    "\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"DEP_DEL15\", maxIter=100, regParam=0.1)\n",
    "\n",
    "# Fit the model on training data\n",
    "svm_model = svm.fit(train_data)\n",
    "\n",
    "# Predictions\n",
    "predictions = svm_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"DEP_DEL15\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3963af-9882-4798-971d-374592688973",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940fca1-a2c3-477d-aa50-fb67b3beab8a",
   "metadata": {},
   "source": [
    "### Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "905d45c1-29db-4288-8c0c-2cabbfeae520",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(data_with_feature_column_df.select(\"features\", \"DEP_DEL15\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34db780-74ce-4c51-8fda-cd3bd77ff588",
   "metadata": {},
   "source": [
    "### Fit, train and predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "67654b31-00ad-4679-b1d3-9bd74353c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mate/.local/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 372:=====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.816019\n",
      "Confusion Matrix:\n",
      "[[48913.     0.]\n",
      " [11028.     0.]]\n",
      "Precision: 0.8160190854340101\n",
      "Recall: 1.0\n",
      "F1 Score: 0.7333481824431576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# RandomForest setup (assuming train_data and test_data are already prepared)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"DEP_DEL15\", numTrees=10)\n",
    "rf_model = rf.fit(train_data)\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"DEP_DEL15\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Compute confusion matrix\n",
    "rdd = predictions.select(col(\"prediction\"), col(\"DEP_DEL15\").cast(\"double\")).rdd\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544459ec-e06f-4dda-a925-8068eaec1665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
